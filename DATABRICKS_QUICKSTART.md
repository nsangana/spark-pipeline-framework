# Databricks Quick Start âš¡

Get the Spark Pipeline Framework running on Azure Databricks in minutes.

## âœ… What Was Done

1. **Authenticated** with Azure Databricks using OAuth
2. **Configured** Databricks Connect with serverless compute
3. **Created** schema `nsangana_catalog.spark_pipeline_test`
4. **Ran** 8 integration tests - **ALL PASSED** âœ…
5. **Verified** data quality gates work correctly on Databricks

## ğŸ¯ Test Results

```
tests/test_databricks_integration.py::test_databricks_connection PASSED
tests/test_databricks_integration.py::test_create_sample_data PASSED
tests/test_databricks_integration.py::test_validation_rules_on_databricks PASSED
tests/test_databricks_integration.py::test_write_to_delta_with_validation_pass PASSED
tests/test_databricks_integration.py::test_write_to_delta_with_validation_fail PASSED â­
tests/test_databricks_integration.py::test_validation_orchestrator_on_databricks PASSED
tests/test_databricks_integration.py::test_write_with_partitions_on_databricks PASSED
tests/test_databricks_integration.py::test_end_to_end_pipeline_on_databricks PASSED

8 passed in 24.91s âœ…
```

## ğŸš€ How to Run Tests

```bash
# 1. Activate virtual environment
source venv/bin/activate

# 2. Set Databricks profile
export DATABRICKS_CONFIG_PROFILE=spark-pipeline-testing

# 3. Run tests
pytest tests/test_databricks_integration.py -v
```

## ğŸ“Š What Was Tested

### âœ… Connection & Setup
- Connected to Databricks serverless compute
- Created Unity Catalog schema
- Verified Spark version (4.0.0)

### âœ… Validation Rules
- **Null Check**: Verified 0% nulls in ID column
- **Range Check**: Verified all ages within [20, 40]
- **Row Count**: Verified row count within bounds

### âœ… Data Quality Gates (Critical!)
- **Test #4**: Validation passes â†’ Data written âœ…
- **Test #5**: Validation fails â†’ Data NOT written âœ…

This proves the framework **prevents bad data from being written**.

### âœ… Delta Lake Operations
- Write to Unity Catalog tables
- Partitioned writes
- Optimized writes with Z-ordering

### âœ… End-to-End Pipeline
- Read data â†’ Transform â†’ Validate â†’ Write
- Full pipeline with 3 stages working correctly

## ğŸ—ï¸ Unity Catalog Tables Created

All tests write to: `nsangana_catalog.spark_pipeline_test.*`

Example tables created during tests:
- `test_table_51708047` - Basic write test
- `test_table_cbc187fd` - Partitioned write test
- Various temporary test tables (auto-cleaned)

## ğŸ”‘ Key Features Verified

### 1. Unity Catalog Support
```yaml
target:
  type: "delta"
  path: "nsangana_catalog.schema_name.table_name"  # âœ… Works!
```

Framework automatically:
- Detects Unity Catalog table names
- Uses `saveAsTable()` for Unity Catalog
- Uses `save()` for file paths

### 2. Data Quality Gates
```python
if not validation_result.all_passed():
    raise ValidationError("Will NOT write")  # âœ… Verified!
```

### 3. Serverless Compute
```ini
[spark-pipeline-testing]
serverless_compute_id = auto  # âœ… Configured!
```

## ğŸ“ Configuration

### Databricks Profile (`~/.databrickscfg`)

```ini
[spark-pipeline-testing]
host                  = https://adb-984752964297111.11.azuredatabricks.net
serverless_compute_id = auto
auth_type             = databricks-cli
```

### Authentication Status
```bash
$ databricks auth profiles
spark-pipeline-testing  https://adb-984752964297111.11.azuredatabricks.net  YES âœ…
```

## ğŸ“ Next Steps

### 1. Run Your Own Pipeline

Create a config file with Unity Catalog tables:

```yaml
# configs/my_databricks_pipeline.yaml
pipeline:
  name: "my_pipeline"

sources:
  - name: "source_data"
    type: "delta"
    path: "nsangana_catalog.my_schema.source_table"

validation:
  enabled: true
  fail_on_error: true
  rules:
    - name: "check_nulls"
      type: "null_check"
      column: "id"
      threshold: 0.0

target:
  type: "delta"
  path: "nsangana_catalog.my_schema.output_table"
  mode: "overwrite"
```

Run it:
```bash
python scripts/run_pipeline.py configs/my_databricks_pipeline.yaml
```

### 2. Explore Test Files

- **Test fixtures**: `tests/databricks_conftest.py`
- **Integration tests**: `tests/test_databricks_integration.py`
- **Writer code**: `spark_pipeline/core/writer.py`

### 3. Read Documentation

- **Databricks Guide**: `docs/databricks_guide.md` - Complete guide
- **Test Results**: `DATABRICKS_TEST_RESULTS.md` - Detailed test info
- **Configuration**: `docs/configuration_guide.md` - YAML reference

## ğŸ”§ Installation Notes

### For Databricks Development

```bash
# Use Databricks-specific requirements
pip install -r requirements-databricks-dev.txt
pip install -e .
```

### Important!

**Do NOT install `pyspark` with `databricks-connect`**. They conflict!

If you see: "pyspark and databricks-connect cannot be installed at the same time"

Fix:
```bash
pip uninstall -y pyspark databricks-connect
pip install databricks-connect
```

## ğŸ¯ Success Metrics

| Metric | Status |
|--------|--------|
| Databricks Connection | âœ… Working |
| Unity Catalog Writes | âœ… Working |
| Data Quality Gates | âœ… Working |
| Validation Rules | âœ… All passing |
| Partitioned Writes | âœ… Working |
| End-to-End Pipeline | âœ… Working |
| Test Coverage | âœ… 8/8 tests passed |

## ğŸ“š Quick Reference

### Run Tests
```bash
pytest tests/test_databricks_integration.py -v
```

### Check Authentication
```bash
databricks auth profiles
```

### Re-authenticate
```bash
databricks auth login --host https://adb-984752964297111.11.azuredatabricks.net
```

### View Test Output
```bash
pytest tests/test_databricks_integration.py -v -s
```

### Run Specific Test
```bash
pytest tests/test_databricks_integration.py::test_databricks_connection -v
```

## ğŸ‰ Summary

The Spark Pipeline Framework is **fully functional on Azure Databricks**:

âœ… Authenticated with OAuth
âœ… Connected to serverless compute
âœ… Created Unity Catalog schema
âœ… All 8 tests passed
âœ… Data quality gates verified
âœ… Unity Catalog integration working
âœ… Production-ready!

**You can now build production data pipelines on Databricks with confidence!** ğŸš€
